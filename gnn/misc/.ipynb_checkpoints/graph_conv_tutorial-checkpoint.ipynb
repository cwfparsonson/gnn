{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-do\n",
    "* Add images from paper\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolutional Network (GCN)\n",
    "\n",
    "### Summary\n",
    "* 'Semi-Supervised Classification with Graph Convolutional Networks', T. Kipf & M. Welling, ICLR 2017, https://arxiv.org/abs/1609.02907\n",
    "* DGL tutorial: https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/1_gcn.html\n",
    "* Learned hidden representations at each node encode:\n",
    "    1. Local graph structure\n",
    "    2. Node features\n",
    "* Scales linearly with number of graph edges\n",
    "* Was big step forward in spatial GNN (as opposed to spectral) becoming superior method for ML over large graphs\n",
    "\n",
    "### Overview\n",
    "* Uses the *convolution* operation to *aggregate*/*reduce* messages given to each node in graph after message passing\n",
    "* In most GNNs, each GNN layer (where can stack multiple GNN layers to produce final output) performs 3 stages:\n",
    "    1. Message function: Defined on each edge in the input graph. Generates a message (some representation) to pass from a source node to some set of destination nodes. Each node stores the messages it receives in its mailbox (memory).\n",
    "    2. Reduce/aggregate function: Defined on each node in the input graph. Accesses the node's mailbox of messages and applies some function (e.g. mean, max, min, sum, single-layer NN, multi-layer NN, LSTM, etc.) to reduce the messages into an intermediate *aggregate representation*.\n",
    "    3. NN layer forward pass: Pass the intermediate aggregate representation through a NN model (e.g. a single linear layer) (+ a non-linear/activation function if desired) to produce a final *embedded representation* for each node of the input graph (representing the (1) node features and (2) local graph structure of the original input graph).\n",
    "    \n",
    "    \n",
    "* A discrete convolution operation can be defined as:\n",
    "\n",
    "$$ s(t) = \\frac{1}{m} \\sum_{i=1}^{m} x_{i}(a) \\cdot w_{i} (t-a) $$\n",
    "\n",
    "$s(t)$: Feature map (output of convolution)\n",
    "\n",
    "$x(a)$: Input data (input graph's node features; the input to the NN layer)\n",
    "\n",
    "$w(a)$: Kernel (muti-dimensional array of tuneable parameters/weights of the NN layer)\n",
    "\n",
    "* To implement convolution, GCN defined the following to occur in each GCN layer:\n",
    "    1. Message function: Send (current) hidden representation of each source node's features to destination nodes\n",
    "    2. Reduce/aggregate function: At each destination node, sum all received messages (hidden node representations of surrounding nodes) to get single intermediate aggregate representation at each node\n",
    "    3. NN layer forward pass: Collect the aggregated representations at each node into a vector and pass the vector as inputs through a linear NN layer followed by a relu non-linear activation function\n",
    "* The above is summarised by the formula:\n",
    "\n",
    "$$ H^{l+1} = \\sigma (\\tilde{D}^{\\frac{1}{2}} \\tilde(A) \\tilde(D)^{-\\frac{1}{2}} H^{l} W^{l}) $$\n",
    "\n",
    "$l$: GNN layer\n",
    "\n",
    "$H$: Hidden node representations of current GNN layer\n",
    "\n",
    "$D$: Input graph degree matrix\n",
    "\n",
    "$A$: Input graph adjacency matrix\n",
    "\n",
    "$\\tilde{}$: Denotes a 'renormalisation trick' where add a self-connection to each node of the graph and build the corresponding degree and adjacency matrix\n",
    "\n",
    "$W$: Weight matrix of the NN layer in current GNN layer\n",
    "\n",
    "$\\sigma$: Non-linearity (relu was used)\n",
    "    \n",
    "* In the first GCN layer, the input $H^{0}$ is an $N \\times D$ tensor, where $N$ is the number of nodes in the input graph and $D$ is the number of features on each node. Can stack multiple GNN layers, with the final node-level representation output by the last GNN layer being an $N \\times F$ tensor, where $F$ is the number of possible classes. Can then pass representation through e.g. a softmax layer to classify each node into one of the $F$ possible classes. N.B. If wanted to do graph-level representation, would instead pass final GNN layer output through a *readout function* (usually another trainable NN layer) which takes node-level representations and outputs a graph-level logit representation with number of dimensions equal to the number of possible graph classes, which itself can then be passed through e.g. a softmax function to classify the whole graph.\n",
    "\n",
    "*Insert Figure 1*\n",
    "\n",
    "### Paper Implementation Details\n",
    "* Tackled a **semi-supervised learning** problem where only some of the input data have labels. Used **citation datasets** (**Cora, Citesser, Pubmed**) where each dataset was given a **label rate** (fraction defining fraction of labeled nodes that were used for training (e.g. was 0.052 for Cora, meaning 5% of labeled nodes were used for training, therefore 'semi-supervised'))\n",
    "\n",
    "*Insert Table 1*\n",
    "\n",
    "* Used **categorical cross-entropy loss function**\n",
    "* Used **2 GCN layers**, so had 2 NN layers in total (with weights $W^{0}$ and $W^{1}$ respectively) which were optimised using **full-batch gradient descent** (viable so long as the dataset fits in memory) (from paper: 'We leave memory-efficient extensions with mini-batch stochastic gradient descent for future work')\n",
    "* Stochasticity/regularisation in training process introduced via **dropout_rate = 0.5** and **L2_regularisation_parameter = $5 \\times 10^{-4}$** for *all* GCN layers\n",
    "* Used **Adam optimiser** (**learning rate = 0.01**) trained with a maximum **num_epochs = 200**, where training was stopped with **window_size = 10** (i.e. stop training if validation loss does not decrease for 10 consecutive epochs)\n",
    "* NN in each GNN layer had **16 hidden units**\n",
    "* All test accuracies reported were the mean accuracies from **100 runs**\n",
    "\n",
    "*Insert Table 2*\n",
    "\n",
    "* For large graphs (1 million < num_edges <= 10 million), GPU ran out of memory, but CPU could still handle and train in reasonable times. This paper was before GraphSage, which was first paper to implement mini-batching and neighbourhood sampling for GNNs thereby by-passing this memory/scalability issue\n",
    "\n",
    "*Insert Figure 2*\n",
    "\n",
    "* Tested using 1-10 GCN layers. Found that if didn't add **residual connections**, can have GCN model up to **2 layers deep** before test accuracy drops off dramatically (since context size for each node increase by the size of its Kth order neighborhood for a GNN model with K layers), and that even with residual connections, test accuracy stays relatively constant, so no point adding more than 2 GNN layers and therefore no need for residual connections. Suggested reason for this is due to *overfitting* issues as the number of parameters increases with model depth.\n",
    "\n",
    "*Insert Figure 5*\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow and DGL Implementation of GCN\n",
    "\n",
    "### Define GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: tensorflow\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a simple fully-connected NN layer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class Linear(Layer):\n",
    "    '''Fully-connected (FC) linear layer.'''\n",
    "    \n",
    "    def __init__(self, units, bias=True, activation=None):\n",
    "        super(Linear, self).__init__()\n",
    "        \n",
    "        self.units = units # num units in layer == num dimensions to output (therefore final layer needs units == num_classes)\n",
    "        self.bias = bias # whether or not to bias units in layer\n",
    "        self.activation = activation # (optional) activation function/non-linearity to pass output of linear operation\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        '''Initialises layer weights & biases according to shape of input.'''\n",
    "        # weights\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(input_shape[-1], self.units), dtype='float32'), \n",
    "                             trainable=True)\n",
    "        \n",
    "        # biases\n",
    "        if self.bias:\n",
    "            b_init = tf.zeros_initializer()\n",
    "            self.b = tf.Variable(initial_value=b_init(shape=(self.units,), dtype='float32'),\n",
    "                                 trainable=True)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "    def call(self, inputs):\n",
    "        '''Performs forward propagation through layer when layer called.\n",
    "        \n",
    "        When called for the first time, will automatically run self.build().\n",
    "        \n",
    "        '''\n",
    "        # linear operation\n",
    "        if self.b is not None:\n",
    "            h = tf.matmul(inputs, self.w) + self.b\n",
    "        else:\n",
    "            h = tf.matmul(inputs, self.w)\n",
    "            \n",
    "        # activation function/non-linearity\n",
    "        if self.activation is None:\n",
    "            pass\n",
    "        elif self.activation == 'relu':\n",
    "            h = tf.nn.relu(features=h)\n",
    "        elif self.activation == 'leaky_relu':\n",
    "            h = tf.nn.leaky_relu(features=h)\n",
    "        else:\n",
    "            raise Exception('Invalid \\'activation\\' argument: {}.'.format(activation))\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recall that a GCN layer performs the following stages:\n",
    "    1. Message function: Send (current) hidden representation of each source node's features to destination nodes\n",
    "    2. Reduce/aggregate function: At each destination node, sum all received messages (hidden node representations of surrounding nodes) to get single intermediate aggregate representation at each node\n",
    "    3. NN layer forward pass: Collect the aggregated representations at each node into a vector and pass the vector as inputs through a linear NN layer followed by a relu non-linear activation function\n",
    "* Define a GCN layer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class GCNLayer(Model):\n",
    "    def __init__(self, out_feats, activation):\n",
    "        super(GCNLayer, self).__init__()\n",
    "                \n",
    "        # define message & reduce functions as usual\n",
    "        self.gcn_message_func = dgl.function.copy_src(src='h', out='m') # simply pass (current) hidden node features h from source to mailbox m of destination node(s)\n",
    "        self.gcn_reduce_func = dgl.function.sum(msg='m', out='h') # sum the received messages at each destination node to generate a new aggregate intermediate hidden representation h for each node\n",
    "        \n",
    "        # init fully connected linear layer\n",
    "        self.linear = Linear(units=out_feats, bias=True, activation=activation)\n",
    "        \n",
    "    def call(self, g, features):\n",
    "        # use local scope so stored ndata and edata (e.g. h ndata) automatically popped out when scope exits\n",
    "        with g.local_scope():\n",
    "            # enter node features\n",
    "            g.ndata['h'] = features\n",
    "            \n",
    "            # pass messages between nodes & aggregate messages\n",
    "            g.update_all(self.gcn_message_func, self.gcn_reduce_func)\n",
    "            \n",
    "            # retrieve new intermediate aggregate representation for each node\n",
    "            h = g.ndata['h']\n",
    "            \n",
    "            # forward aggregated messages through FC NN layer to get GNN layer's hidden node representation\n",
    "            h = self.linear(inputs=h)\n",
    "            \n",
    "            return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stack GCN layers to create a GCN model which outputs logits, which can then be passed through a e.g. softmax function to get node predictions/classifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(Model):\n",
    "    '''Implementation of convolutional graph neural network.\n",
    "    \n",
    "    Must configure input, hidden and output layers by using the layers_config\n",
    "    dict. Specify out_feats for each layer to specify the number of output\n",
    "    features each layer should output. The model will automatically set the\n",
    "    number of input features for each layer based on what is being passed into\n",
    "    the layer when called. N.B. NN must have minimum of 1 layer. N.B.2. The number\n",
    "    of elements given to out_feats and activations lists should be equal and\n",
    "    will correspond to the total number of layers (input, hidden and output)\n",
    "    in the model.\n",
    "    \n",
    "    The final layer (the output layer) should have activation==None so that\n",
    "    the model outputs logits, which may then be externally converted into \n",
    "    probability predictions using e.g. softmax.\n",
    "    N.B. The final value in out_feats arg list should be the number of classes\n",
    "    being classified by the output layer (e.g. if have 7 classes to classify \n",
    "    inputs into, final out_feats val should be 7).\n",
    "    \n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 layers_config={'out_feats': [16, 7],\n",
    "                                'activations': ['relu', None]}):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.model_name = 'graph_conv'\n",
    "\n",
    "        assert len(layers_config['out_feats']) >= 1, \\\n",
    "                'Must specify out_feats for >=1 layer(s)'\n",
    "        assert len(layers_config['out_feats']) == len(layers_config['activations']), \\\n",
    "                'Must specify out_feats and activations for all layers \\\n",
    "                (have specified {} out_feats and {} activations)'.format(len(layers_config['out_feats']),\n",
    "                                                                         len(layers_config['activations']))\n",
    "        assert layers_config['activations'][-1] is None, \\\n",
    "                'Final layer must have activation as None to output logits'\n",
    "\n",
    "        # stack GCN layers\n",
    "        self._layers = []\n",
    "        n_layers = len(layers_config['out_feats'])\n",
    "        for i in range(n_layers):\n",
    "            self._layers.append(GCNLayer(out_feats=layers_config['out_feats'][i],\n",
    "                                         activation=layers_config['activations'][i]))\n",
    "\n",
    "        self.num_layers = len(self._layers)\n",
    "\n",
    "    def call(self, g, features):\n",
    "        '''Forward graph node features through all GNN layers, generating feature & local structure representations for each node.'''\n",
    "        \n",
    "        h = features\n",
    "        for layer in self._layers:\n",
    "            h = layer(g, h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache failed, re-processing.\n",
      "Finished data loading and preprocessing.\n",
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done saving data into cached files.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def unpack_dataset(dataset):\n",
    "    graph = dataset[0]\n",
    "    features = graph.ndata['feat']\n",
    "    labels = graph.ndata['label']\n",
    "    train_mask = graph.ndata['train_mask']\n",
    "    val_mask = graph.ndata['val_mask']\n",
    "    test_mask = graph.ndata['test_mask']\n",
    "\n",
    "    return graph, features, labels, train_mask, val_mask, test_mask\n",
    "\n",
    "def load_data(dataset='cora'):\n",
    "    if dataset == 'cora':\n",
    "        data = dgl.data.CoraGraphDataset()\n",
    "    elif dataset == 'citeseer':\n",
    "        data = dgl.data.CiteseerGraphDataset()\n",
    "    elif dataset == 'pubmed':\n",
    "        data = dgl.data.PubmedGraphDataset()\n",
    "    elif dataset == 'reddit':\n",
    "        data = dgl.data.RedditDataset()\n",
    "    else:\n",
    "        raise ValueError('Unknown dataset: {}'.format(dataset))\n",
    "\n",
    "    return unpack_dataset(data)\n",
    "\n",
    "g, features, labels, train_mask, val_mask, test_mask = load_data('cora')\n",
    "\n",
    "# one-hot encode labels\n",
    "num_classes = int(len(np.unique(labels)))\n",
    "onehot_labels = tf.one_hot(indices=labels, depth=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cora dataset is made up of 2,708 scientific publications (modelled as nodes) classified into 1 of 7 possible classes. Where one paper has cited another, an edge has been added between the nodes/papers, producing 10,556 edges in total. Each node has a set of features, where the features are a 0/1 valued word vector indicating the absence/presence of the corresponding word in the paper from a dictionary, and where the dictionary has been defined as 1,433 unique words.\n",
    "\n",
    "The train, test and validation masks are lists of boolean values for each node, separating the dataset into train, test and validation sets.\n",
    "\n",
    "About 5% of the 2,708 labeled nodes are used in the training set, making this a so-called 'semi-supervised' task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(2708, 1433)\n",
      "\n",
      "Labels:\n",
      "[3 4 4 ... 3 3 3]\n",
      "(2708,)\n",
      "\n",
      "Train Mask:\n",
      "[ True  True  True ... False False False]\n",
      "(2708,)\n",
      "\n",
      "Validation Mask:\n",
      "[False False False ... False False False]\n",
      "(2708,)\n",
      "\n",
      "Test Mask\n",
      "[False False False ...  True  True  True]\n",
      "(2708,)\n"
     ]
    }
   ],
   "source": [
    "print('\\nFeatures:\\n{}'.format(features))\n",
    "print(features.numpy().shape)\n",
    "\n",
    "print('\\nLabels:\\n{}'.format(labels))\n",
    "print(labels.numpy().shape)\n",
    "\n",
    "print('\\nTrain Mask:\\n{}'.format(train_mask))\n",
    "print(train_mask.numpy().shape)\n",
    "\n",
    "print('\\nValidation Mask:\\n{}'.format(val_mask))\n",
    "print(val_mask.numpy().shape)\n",
    "\n",
    "print('\\nTest Mask\\n{}'.format(test_mask))\n",
    "print(test_mask.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the Cora graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "networkx_g = g.to_networkx()\n",
    "pos = nx.kamada_kawai_layout(networkx_g)\n",
    "\n",
    "fig_scale=1\n",
    "fig = plt.figure(figsize=[15*fig_scale,15*fig_scale])\n",
    "\n",
    "# nodes\n",
    "nx.draw_networkx_nodes(networkx_g,\n",
    "                       pos,\n",
    "                       nodelist=networkx_g.nodes(),\n",
    "                       node_size=12,\n",
    "                       node_color='#5293fa',\n",
    "                       label='publication')\n",
    "# edges\n",
    "nx.draw_networkx_edges(networkx_g,\n",
    "                       pos,\n",
    "                       edgelist=networkx_g.edges(),\n",
    "                       edge_color='#cedff0',\n",
    "                       arrows=False,\n",
    "                       width=0.5,\n",
    "                       alpha=0.6,\n",
    "                       label='citation')\n",
    "\n",
    "plt.legend(labelspacing=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Evaluation Function\n",
    "\n",
    "* Need a function to use to test the model on both the validation and the test data set(s):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, g, features, labels, mask):\n",
    "    logits = model(g, features)\n",
    "    if type(mask[0].numpy()) == np.bool_:\n",
    "        logits = tf.boolean_mask(tensor=logits, mask=mask)\n",
    "        labels = tf.boolean_mask(tensor=labels, mask=mask)\n",
    "    else:\n",
    "        logits = tf.gather(logits, mask)\n",
    "        labels = tf.gather(labels, mask)\n",
    "    indices = tf.math.argmax(logits, axis=1)\n",
    "    indices = tf.one_hot(indices=indices, depth=len(labels[0]))\n",
    "    correct = 0\n",
    "    for i in range(len(labels)):\n",
    "        if np.array_equal(indices.numpy()[i], labels.numpy()[i]):\n",
    "            correct+=1\n",
    "    acc = correct / len(labels)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test the GNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    # init gnn model\n",
    "    layers_config = {'out_feats': [32, num_classes],\n",
    "                     'activations': ['relu', None]}\n",
    "    model = GCN(layers_config=layers_config)\n",
    "\n",
    "    # add edges between each node and itself to preserve old node representations\n",
    "    g.add_edges(g.nodes(), g.nodes())\n",
    "\n",
    "    # define optimiser\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "    # run training loop\n",
    "    all_loss = []\n",
    "    all_acc = []\n",
    "    all_epochs = []\n",
    "    all_logits = []\n",
    "    num_epochs = 200\n",
    "    for epoch in range(num_epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(g, features)\n",
    "            all_logits.append(logits)\n",
    "            loss = tf.nn.softmax_cross_entropy_with_logits(labels=tf.boolean_mask(tensor=onehot_labels, mask=train_mask),\n",
    "                                                           logits=tf.boolean_mask(tensor=logits, mask=train_mask))\n",
    "            all_loss.append(tf.keras.backend.mean(loss))\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        acc = evaluate(model, g, features, onehot_labels, val_mask)\n",
    "        all_acc.append(acc)\n",
    "        all_epochs.append(epoch)\n",
    "        print('Epoch: {} | Training loss: {} | Validation accuracy: {}'.format(epoch, tf.keras.backend.mean(loss), acc))\n",
    "\n",
    "    acc = evaluate(model, g, features, onehot_labels, test_mask)\n",
    "    print('Final test accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first re-plot the Cora dataset, but now colouring the nodes by their ground truth label/category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def visualise_node_labels(net, pos, _labels, num_classes, fig_scale=1):\n",
    "    fig = plt.figure(figsize=[15*fig_scale,15*fig_scale])\n",
    "    \n",
    "    # nodes\n",
    "    category_colours = iter(sns.color_palette(palette='hls', n_colors=num_classes, desat=None))\n",
    "    colour_dict = {i: next(category_colours) for i in range(num_classes)}\n",
    "    \n",
    "    node_ids = net.nodes()\n",
    "    node_class_dict = {_class: [] for _class in range(num_classes)}\n",
    "    labs = iter(_labels)\n",
    "    for node_id in node_ids:\n",
    "        lab = next(labs)\n",
    "        node_class_dict[lab].append(node_id)\n",
    "    \n",
    "    for _class in node_class_dict.keys():\n",
    "        nx.draw_networkx_nodes(networkx_g,\n",
    "                               pos,\n",
    "                               nodelist=node_class_dict[_class],\n",
    "                               node_size=12,\n",
    "                               node_color=colour_dict[_class],\n",
    "                               label='class_'+str(_class))\n",
    "    # edges\n",
    "    nx.draw_networkx_edges(networkx_g,\n",
    "                           pos,\n",
    "                           edgelist=networkx_g.edges(),\n",
    "                           edge_color='#cedff0',\n",
    "                           arrows=False,\n",
    "                           width=0.5,\n",
    "                           alpha=0.6)\n",
    "    \n",
    "    # legend\n",
    "    plt.legend(labelspacing=2)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "visualise_node_labels(networkx_g, pos, _labels=labels.numpy(), num_classes=num_classes, fig_scale=1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logits (final output of the GNN model) have been saved for each epoch, with the first list of logits being the output of the GNN after 1 epoch and the final list the output after the final epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(all_logits).shape)\n",
    "print(all_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert logits into predictions, pass them through e.g. a softmax function to get a probability distribution across all possible classes. For the Cora dataset, there are 7 possible classes, hence why for 2,708 nodes, the shape of the GNN's output logits tensor is 2708 x 7 (and will have 200 of these if collecting across all 200 epochs). The class with the highest probability should be considered that predicted by the GNN, therefore we just find the maximum argument along axis=1 for each row (input node) in the softmax output tensor.\n",
    "\n",
    "GNN model's node category predictions after first epoch of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = tf.nn.softmax(all_logits[0])\n",
    "predictions = np.argmax(probabilities.numpy(), axis=1)\n",
    "\n",
    "visualise_node_labels(networkx_g, pos, _labels=predictions, num_classes=num_classes, fig_scale=fig_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, after just 1 epoch, the GNN model's predictions are quite different from the ground truth node labels.\n",
    "\n",
    "GNN model's predictions after final epoch of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = tf.nn.softmax(all_logits[-1])\n",
    "predictions = np.argmax(probabilities.numpy(), axis=1)\n",
    "\n",
    "visualise_node_labels(networkx_g, pos, _labels=predictions, num_classes=num_classes, fig_scale=fig_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, after training through all 200 epochs, the GNN's node predictions are much closer to the ground truth labels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_scheduler",
   "language": "python",
   "name": "deep_scheduler"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
